<!DOCTYPE html>
<html lang="en">

  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-116924853-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-116924853-1');
</script>
<style>
#video {
  position: relative;
  width: 100%;
  height: 0;
  padding-bottom: 56.25%;
}
 #video iframe {
   position: absolute;
   width: 100%;
   height: 100%;
   left: 0;
   top: 0;
 } 
</style>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Hazel Doughty</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:100,200,300,400,500,600,700,800,900" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i" rel="stylesheet">
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet">
    <link href="vendor/devicons/css/devicons.min.css" rel="stylesheet">
    <link href="vendor/simple-line-icons/css/simple-line-icons.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="css/resume.min.css" rel="stylesheet">

  </head>

  <body id="page-top">

    <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
      <a class="navbar-brand js-scroll-trigger" href="#page-top">
        <span class="d-block d-lg-none">Hazel Doughty</span>
        <span class="d-none d-lg-block">
          <img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="img/profile.png" alt="">
        </span>
      </a>
      <span class="d-none d-lg-block name">Hazel Doughty</span>
      <span class="d-none d-lg-block">
          <p>Third year PhD student in Computer Vision at the University of Bristol.</p>
          <p>hazel.doughty *at* bristol.ac.uk</p>
          <p>Visual Information Laboratory
            One Cathedral Square, BS1 5DD
            Bristol</p>
        </span>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarSupportedContent">
        <ul class="navbar-nav">
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#about">About</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#projects">Projects</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#publications">Publications</a>
          </li>
          </li>
        </ul>
      </div>
    </nav>

    <div class="container-fluid p-0">

      <section class="resume-section p-3 p-lg-5 d-flex d-column" id="about">
        <div class="my-auto">
          <!--<h1 class="mb-0">Hazel
            <span class="text-primary">Doughty</span>
          </h1>-->
          <h2 class="mb-5">About</h2>
          <p class="mb-5">I'm a second year PhD student in Computer Vision at the University of Bristol. My area of interest is Video Understanding, specifically in Skill Determination and Egocentric Video. I'm supervised by <a href="http://www.bris.ac.uk/engineering/people/walterio-w-mayol-cuevas/index.html">Prof. Walterio Mayol-Cuevas</a> and <a href="https://dimadamen.github.io/">Dr. Dima Damen</a> and began my PhD in September 2016. I completed my MEng in Computer Science from the University of Bristol in June 2016.</p>

        </div>
      </section>

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="projects">
        <div class="my-auto">
          <h2 class="mb-5">Projects</h2>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="col-md-6 resume-content mr-auto p-2">
              <h3 class="mb-0">The Pros and Cons: Rank-aware Temporal Attention for Skill Determination in Long Videos</h3>
              <div class="subheading mb-3">Hazel Doughty, Walterio Mayol-Cuevas and Dima Damen in IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2019.</div>
              <p>     We present a new model to determine relative skill from long videos, through learnable temporal attention modules. Previous work formulates skill determination for common tasks as a ranking problem, yet measures skill from randomly sampled video segments. We believe this approach to be limiting since many parts of the video are irrelevant to assessing skill, and there may be variability in the skill exhibited throughout a video. Assessing skill from a single section may not reflect the overall skill in the video. We propose to train rank-specific temporal attention modules, learned with only video-level supervision, using a novel rank-aware loss function. In addition to attending to task-relevant video parts, our proposed loss jointly trains two attention modules to separately attend to video parts which are indicative of higher (pros) and lower (cons) skills. We evaluate the approach on the public EPIC-Skills dataset and additionally collect and annotate a larger dataset for skill determination with five previously unexplored tasks. Our method outperforms previous approaches and classic softmax attention on both datasets by over 4% pairwise accuracy, and as much as 12% on individual tasks. We also demonstrate our model's ability to attend to rank-aware parts of the video. </p>
<p><a href="https://arxiv.org/abs/1812.05538">[arXiv]</a> <a href="https://github.com/hazeld/rank-aware-attention-network">[Dataset]</a> <a href="Papers/TheProsandCons/index.html">[Webpage]</a>
            </div>
            <div class="col-md-6 p-2 embed-responsive">
              <div id="video">
              <iframe class="embed-responsive-item" width="100%" height="auto" max-width="560" max-height="315" src="https://www.youtube.com/embed/eSJsiqOQTKc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
              </div>
            </div>
          </div>
        </div>
        <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="col-md-6 resume-content mr-auto p-2">
              <h3 class="mb-0">Scaling Egocentric Vision: The EPIC-KITCHENS Dataset</h3>
              <div class="subheading mb-3">Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, Michael Wray in European Conference on Computer Vision (<b>ECCV</b>), 2018. (Oral) </div>
              <p> First-person vision is gaining interest as it offers a unique viewpoint on people's interaction with objects, their attention, and even intention. However, progress in this challenging domain has been relatively slow due to the lack of sufficiently large datasets. In this paper, we introduce EPIC-KITCHENS, a large-scale egocentric video benchmark recorded by 32 participants in their native kitchen environments. Our videos depict nonscripted daily activities: we simply asked each participant to start recording every time they entered their kitchen. Recording took place in 4 cities (in North America and Europe) by participants belonging to 10 different nationalities, resulting in highly diverse cooking styles. Our dataset features 55 hours of video consisting of 11.5M frames, which we densely labeled for a total of 39.6K action segments and 454.3K object bounding boxes. Our annotation is unique in that we had the participants narrate their own videos (after recording), thus reflecting true intention, and we crowd-sourced ground-truths based on these. We describe our object, action and anticipation challenges, and evaluate several baselines over two test splits, seen and unseen kitchens.</p>
<p><a href="https://arxiv.org/abs/1804.02748">[arXiv]</a> <a href="https://epic-kitchens.github.io/2018">[Webpage & Dataset]</a>
<p></p>
<p></p>
          </div>
          <div class="col-md-6 p-2 embed-responsive">
            <div id="video">
              <iframe class="embed-responsive-item" width="100%" height="auto" max-width="560" max-height="315" src="https://www.youtube.com/embed/Dj6Y3H0ubDw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
              </div>
            </div>
        </div>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="col-md-6 resume-content mr-auto p-2">
              <h3 class="mb-0">Who's Better? Who's Best? Pairwise Deep Ranking for Skill Determination </h3>
              <div class="subheading mb-3">Hazel Doughty, Dima Damen and Walterio Mayol-Cuevas in IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2018. (Spotlight)</div>
              <p> We present a method for assessing skill from video, applicable to a variety of tasks, ranging from surgery to drawing and rolling pizza dough. We formulate the problem as pairwise (<i>who's better?</i>) and overall (<i>who's best?</i>) ranking of video collections, using supervised deep ranking.
We propose a novel loss function that learns discriminative features when a pair of videos exhibit variance in skill, and learns shared features when a pair of videos exhibit comparable skill levels. 
Results demonstrate our method is applicable across tasks, with the percentage of correctly ordered pairs of videos ranging from 70% to 83% for four datasets.
We demonstrate the robustness of our approach via sensitivity analysis of its parameters. 

We see this work as effort toward the automated organization of how-to video collections and overall, generic skill determination in video.</p>
<p><a href="https://arxiv.org/abs/1703.09913">[arXiv]</a> <a href="bib/whos_better.html">[Bibtex]</a> <a href="https://drive.google.com/file/d/1oX0dPM5IP638nB0YHt4L70aigIdqqpYr/view?usp=sharing">[Dataset]</a>
            </div>
            <div class="col-md-6 p-2 embed-responsive">
              <div id="video">
              <iframe class="embed-responsive-item" width="100%" height="auto" max-width="560" max-height="315" src="https://www.youtube.com/embed/R3QoZ-FltUQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
              </div>
            </div>
          </div>
        </div>

      </section>

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="publications">
        <div class="my-auto">
          <h2 class="mb-5">Publications</h2>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">2018</h3>
              <p>Doughty, H., Mayol-Cuevas, W., Damen, D., 'The Pros and Cons: Rank-aware Temporal Attention for Skill Determination in Long Video'. arXiv preprint arXiv:1812.05538</p>
              <p>Damen, D., Doughty, H., Farinella, G.M., Fidler, S., Furnari, A., Kazakos, E., Moltisanti, D., Munro, J., Perrett, T., Price, W. and Wray, M., 2018. Scaling Egocentric Vision: The EPIC-KITCHENS Dataset. The European Conference on Computer Vision (ECCV). 2018 
              <p>Doughty, H., Damen, D., Mayol-Cuevas, W., 'Who's Better? Who's Best? Pairwise Deep Ranking for Skill Determination'. Computer Vision and Pattern Recognition (CVPR). 2018</p>
            </div>
          </div>

        </div>
      </section>


    </div>

    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

    <!-- Custom scripts for this template -->
    <script src="js/resume.min.js"></script>

  </body>

</html>
