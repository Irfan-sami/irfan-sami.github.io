<!DOCTYPE html>
<html lang="en">

  <head>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-116924853-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-116924853-1');
</script>
<link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css"/>
    <link rel="shortcut icon" type="image/ico" href="favicon.ico" />
<style>
#video {
  position: relative;
  width: 100%;
  height: 0;
  padding-bottom: 56.25%;
}
 #video iframe {
   position: absolute;
   width: 100%;
   height: 100%;
   left: 0;
   top: 0;
 } 
</style>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Hazel Doughty</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:100,200,300,400,500,600,700,800,900" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i" rel="stylesheet">
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet">
    <link href="vendor/devicons/css/devicons.min.css" rel="stylesheet">
    <link href="vendor/simple-line-icons/css/simple-line-icons.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="css/resume.min.css" rel="stylesheet">

  </head>

  <body id="page-top">

    <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
      <a class="navbar-brand js-scroll-trigger" href="#page-top">
        <span class="d-block d-lg-none">Hazel Doughty</span>
        <span class="d-none d-lg-block">
          <img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="img/profile.png" alt="">
        </span>
      </a>
      <span class="d-none d-lg-block name">Hazel Doughty</span>
      <span class="d-none d-lg-block">
          <p>Fourth year PhD student in Computer Vision at the University of Bristol.</p>
          <p>hazel.doughty *at* bristol.ac.uk</p>
        </span>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarSupportedContent">
        <ul class="navbar-nav">
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#about">About</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#projects">Projects</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#publications">Publications</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="HazelDoughtyCV.pdf">CV</a>
          </li>
        </ul>
      </div>
    </nav>

    <div class="container-fluid p-0">

      <section class="resume-section p-3 p-lg-5 d-flex d-column" id="about">
        <div class="my-auto">
          <!--<h1 class="mb-0">Hazel
            <span class="text-primary">Doughty</span>
          </h1>-->
          <h2 class="mb-5">About</h2>
          <p class="mb-5">I'm a fourth year PhD student in Computer Vision at the University of Bristol. My area of interest is Video Understanding, specifically in Skill Determination and Egocentric Video. I'm supervised by <a href="http://www.bris.ac.uk/engineering/people/walterio-w-mayol-cuevas/index.html">Prof. Walterio Mayol-Cuevas</a> and <a href="https://dimadamen.github.io/">Dr. Dima Damen</a> and began my PhD in September 2016. I completed my MEng in Computer Science from the University of Bristol in June 2016.</p>
          <ul class="list-inline list-social-icons mb-0">

              <li class="list-inline-item">
                  <a href="https://scholar.google.com/citations?user=b3koBVwAAAAJ&hl=en">
                      <span class="fa-stack fa-lg">
                          <i class="ai ai-google-scholar-square ai-2x"></i>
                      </span>
                  </a>
              </li>

              <li class="list-inline-item">
                  <a href="https://github.com/hazeld">
                      <span class="fa-stack fa-lg">
                          <i class="fa fa-square fa-stack-2x"></i>
                          <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                      </span>
                  </a>
              </li>

              <li class="list-inline-item">
                  <a href="https://twitter.com/doughty_hazel">
                      <span class="fa-stack fa-lg">
                          <i class="fa fa-square fa-stack-2x"></i>
                          <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
                      </span>
                  </a>
              </li>

              <li class="list-inline-item">
                  <a href="HazelDoughtyCV.pdf">
                      <span class="fa-stack fa-lg">
                          <i class="fa fa-square fa-stack-2x"></i>
                          <!--<i class="fa fa-user fa-stack-1x fa-inverse"></i>--!>
                               
                          <i class="fa fa-id-card fa-stack-1x fa-inverse"></i>
                      </span>
                  </a>
              </li>

                               </ul>
                               <p>&nbsp;</p>
                               <p>&nbsp;</p>
          <h3 class="mb-5">News</h3>
          <ul>
              <li> Feb 2020: <a href="https://arxiv.org/abs/1912.06617">Action Modifers: Learning from Adverbs in Instructional Videos</a> is accepted in CVPR 2020.</li>
              <li> Jan 2020: I'm co-organizing the <a href="https://sites.google.com/view/wicvworkshop-cvpr2020/">Women in Computer Vision</a> and <a href="https://eyewear-computing.org/EPIC_CVPR20/">Egocentric Perception, Interaction and Computing</a> workshops at CVPR 2020.</li>
                  <li> Dec 2019: Our new paper on 'Action Modifiers' is available on <a href="https://arxiv.org/abs/1912.06617">arXiv</a></li>
              <li> June 2019: We're presenting our paper on rank-aware temporal attention for skill determination at CVPR 2019.
          </ul>
        </div>
      </section>

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="projects">
        <div class="my-auto">
          <h2 class="mb-5">Projects</h2>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
              <div class="col-md-6 resume-content mr-auto p-2">
              <h3 class="mb-0">Action Modifiers: Learning from Adverbs in Instrucional Videos</h3>
              <div class="subheading mb-3">Hazel Doughty, Ivan Laptev, Walterio Mayol-Cuevas and Dima Damen in IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2020.</div>
              <p>     We present a method to learn a representation for adverbs from instructional videos using weak supervision from the accompanying narrations. Key to our method is the fact that the visual representation of the adverb is highly dependant on the action to which it applies, although the same adverb will modify multiple actions in a similar way. For instance, while 'spread quickly' and 'mix quickly' will look dissimilar, we can learn a common representation that allows us to recognize both, among other actions. We formulate this as an embedding problem, and use scaled dot-product attention to learn from weakly-supervised video narrations. We jointly learn adverbs as invertible transformations operating on the embedding space, so as to add or remove the effect of the adverb. As there is no prior work on weakly supervised learning from adverbs, we gather paired action-adverb annotations from a subset of the HowTo100M dataset for 6 adverbs: quickly/slowly, finely/coarsely, and partially/completely. Our method outperforms all baselines for video-to-adverb retrieval with a performance of 0.719 mAP. We also demonstrate our model's ability to attend to the relevant video parts in order to determine the adverb for a given action. </p>
<p><a href="Papers/ActionModifiers/">[Webpage]</a> <a href="https://arxiv.org/abs/1912.06617">[arXiv]</a> <a href="https://github.com/hazeld/action-modifiers">[Dataset and Code]</a></p>
              </div>
              <div class="col-md-6 p-2 img-responsive">
                  <img class="img-fluid concept-img" src="img/action_mods_concept.png" style="margin-left:auto; margin-right:auto; display:block; max-width:70%">
              </div>
              </div>
              <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="col-md-6 resume-content mr-auto p-2">
              <h3 class="mb-0">The Pros and Cons: Rank-aware Temporal Attention for Skill Determination in Long Videos</h3>
              <div class="subheading mb-3">Hazel Doughty, Walterio Mayol-Cuevas and Dima Damen in IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2019.</div>
              <p>     We present a new model to determine relative skill from long videos, through learnable temporal attention modules. Previous work formulates skill determination for common tasks as a ranking problem, yet measures skill from randomly sampled video segments. We believe this approach to be limiting since many parts of the video are irrelevant to assessing skill, and there may be variability in the skill exhibited throughout a video. Assessing skill from a single section may not reflect the overall skill in the video. We propose to train rank-specific temporal attention modules, learned with only video-level supervision, using a novel rank-aware loss function. In addition to attending to task-relevant video parts, our proposed loss jointly trains two attention modules to separately attend to video parts which are indicative of higher (pros) and lower (cons) skills. We evaluate the approach on the public EPIC-Skills dataset and additionally collect and annotate a larger dataset for skill determination with five previously unexplored tasks. Our method outperforms previous approaches and classic softmax attention on both datasets by over 4% pairwise accuracy, and as much as 12% on individual tasks. We also demonstrate our model's ability to attend to rank-aware parts of the video. </p>
<p><a href="Papers/TheProsandCons">[Webpage]</a> <a href="https://arxiv.org/abs/1812.05538">[arXiv]</a> <a href="https://github.com/hazeld/rank-aware-attention-network">[Dataset & Code]</a> 
            </div>
            <div class="col-md-6 p-2 embed-responsive">
              <div id="video">
              <iframe  src="https://www.youtube.com/embed/EScWN5SnNr0" class="embed-responsive-item" width="100%" height="auto" max-width="560" max-height="315" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
              </div>
            </div>
          </div>
        </div>
        <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="col-md-6 resume-content mr-auto p-2">
              <h3 class="mb-0">Scaling Egocentric Vision: The EPIC-KITCHENS Dataset</h3>
              <div class="subheading mb-3">Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, Michael Wray in European Conference on Computer Vision (<b>ECCV</b>), 2018. (Oral) </div>
              <p> First-person vision is gaining interest as it offers a unique viewpoint on people's interaction with objects, their attention, and even intention. However, progress in this challenging domain has been relatively slow due to the lack of sufficiently large datasets. In this paper, we introduce EPIC-KITCHENS, a large-scale egocentric video benchmark recorded by 32 participants in their native kitchen environments. Our videos depict nonscripted daily activities: we simply asked each participant to start recording every time they entered their kitchen. Recording took place in 4 cities (in North America and Europe) by participants belonging to 10 different nationalities, resulting in highly diverse cooking styles. Our dataset features 55 hours of video consisting of 11.5M frames, which we densely labeled for a total of 39.6K action segments and 454.3K object bounding boxes. Our annotation is unique in that we had the participants narrate their own videos (after recording), thus reflecting true intention, and we crowd-sourced ground-truths based on these. We describe our object, action and anticipation challenges, and evaluate several baselines over two test splits, seen and unseen kitchens.</p>
<p><a href="https://arxiv.org/abs/1804.02748">[arXiv]</a> <a href="https://epic-kitchens.github.io/2018">[Webpage & Dataset]</a>
<p></p>
<p></p>
          </div>
          <div class="col-md-6 p-2 embed-responsive">
            <div id="video">
              <iframe class="embed-responsive-item" width="100%" height="auto" max-width="560" max-height="315" src="https://www.youtube.com/embed/Dj6Y3H0ubDw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
              </div>
            </div>
        </div>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="col-md-6 resume-content mr-auto p-2">
              <h3 class="mb-0">Who's Better? Who's Best? Pairwise Deep Ranking for Skill Determination </h3>
              <div class="subheading mb-3">Hazel Doughty, Dima Damen and Walterio Mayol-Cuevas in IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2018. (Spotlight)</div>
              <p> We present a method for assessing skill from video, applicable to a variety of tasks, ranging from surgery to drawing and rolling pizza dough. We formulate the problem as pairwise (<i>who's better?</i>) and overall (<i>who's best?</i>) ranking of video collections, using supervised deep ranking.
We propose a novel loss function that learns discriminative features when a pair of videos exhibit variance in skill, and learns shared features when a pair of videos exhibit comparable skill levels. 
Results demonstrate our method is applicable across tasks, with the percentage of correctly ordered pairs of videos ranging from 70% to 83% for four datasets.
We demonstrate the robustness of our approach via sensitivity analysis of its parameters. 

We see this work as effort toward the automated organization of how-to video collections and overall, generic skill determination in video.</p>
<p><a href="https://arxiv.org/abs/1703.09913">[arXiv]</a> <a href="bib/whos_better.html">[Bibtex]</a> <a href="https://drive.google.com/file/d/1oX0dPM5IP638nB0YHt4L70aigIdqqpYr/view?usp=sharing">[Dataset]</a>
            </div>
            <div class="col-md-6 p-2 embed-responsive">
              <div id="video">
              <iframe class="embed-responsive-item" width="100%" height="auto" max-width="560" max-height="315" src="https://www.youtube.com/embed/R3QoZ-FltUQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
              </div>
            </div>
          </div>
        </div>

      </section>

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="publications">
        <div class="my-auto">
          <h2 class="mb-5">Publications</h2>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
              <div class="resume-content mr-auto">
                  <h3 class="mb-0">2020</h3>
                  <p>Doughty, H., Laptev, I., Mayol-Cuevas, W., Damen, D., 'Action Modifiers: Learning from Adverbs in Instructional Videos'. Computer Vision and Pattern Recognition (CVPR). 2020</p>
                  <h3 class="mb-0">2019</h3>
                  <p>Doughty, H., Mayol-Cuevas, W., Damen, D., 'The Pros and Cons: Rank-aware Temporal Attention for Skill Determination in Long Video'. Computer Vision and Pattern Recognition (CVPR). 2019</p>
              <h3 class="mb-0">2018</h3>
              <p>Damen, D., Doughty, H., Farinella, G.M., Fidler, S., Furnari, A., Kazakos, E., Moltisanti, D., Munro, J., Perrett, T., Price, W. and Wray, M., 2018. Scaling Egocentric Vision: The EPIC-KITCHENS Dataset. The European Conference on Computer Vision (ECCV). 2018 
              <p>Doughty, H., Damen, D., Mayol-Cuevas, W., 'Who's Better? Who's Best? Pairwise Deep Ranking for Skill Determination'. Computer Vision and Pattern Recognition (CVPR). 2018</p>
            </div>
          </div>

        </div>
      </section>


    </div>

    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

    <!-- Custom scripts for this template -->
    <script src="js/resume.min.js"></script>

  </body>

</html>
